"""
This file constructs and trains a multi-agent Advantage Actor-Critic (A2C) algorithm to optimally draft a fantasy
football team.
We take as input the Best_Ball_Draft_Board.cvs generated by Best_Ball_Draft_Board.py
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR


class ActorNetwork(nn.Module):
    """Initialize the policy network for the actor."""

    def __init__(self, state_size, action_size, hidden_layers):
        super(ActorNetwork, self).__init__()
        self.network = self.create_actor_layers(state_size, action_size, hidden_layers)

    @staticmethod
    def create_actor_layers(state_size, action_size, hidden_layers):
        layers = []
        input_size = state_size
        for hidden_size in hidden_layers:
            layers.append(nn.Linear(input_size, hidden_size))
            layers.append(nn.ReLU())
            input_size = hidden_size
        layers.append(nn.Linear(input_size, action_size))
        layers.append(nn.Softmax(dim=-1))  # Softmax for probability distribution
        return nn.Sequential(*layers)

    def forward(self, state):
        return self.network(state)


class CriticNetwork(nn.Module):
    """Initialize the value network for the critic."""

    def __init__(self, state_size, hidden_layers):
        super(CriticNetwork, self).__init__()
        self.network = self.create_critic_layers(state_size, hidden_layers)

    @staticmethod
    def create_critic_layers(state_size, hidden_layers):
        layers = []
        input_size = state_size
        for hidden_size in hidden_layers:
            layers.append(nn.Linear(input_size, hidden_size))
            layers.append(nn.ReLU())
            input_size = hidden_size
        layers.append(nn.Linear(input_size, 1))  # Outputs single scalar value
        return nn.Sequential(*layers)

    def forward(self, state):
        return self.network(state)


class A2CAgent:

    def __init__(self, team_id, state_size, action_size, hidden_layers, position_limits, actor_max_norm=1.0,
                 critic_max_norm=1.0, actor_lr=1e-3, critic_lr=5e-4, discount_factor=0.95):
        """ Initialize an individual agent for a team. """
        self.team_id = team_id  # Team identification for this agent
        self.position_limits = position_limits

        # Network hyperparameters
        self.actor_lr = actor_lr
        self.critic_lr = critic_lr
        self.discount_factor = discount_factor
        self.actor_max_norm = actor_max_norm  # Max gradient norm for actor network gradient clipping.
        self.critic_max_norm = critic_max_norm  # Max gradient norm for critic network gradient clipping.

        # Initialize Actor and Critic networks
        self.actor_network = ActorNetwork(state_size, action_size, hidden_layers)
        self.critic_network = CriticNetwork(state_size, hidden_layers)

        # Initialize optimizers
        self.actor_optimizer = optim.AdamW(self.actor_network.parameters(), lr=self.actor_lr)
        self.critic_optimizer = optim.AdamW(self.critic_network.parameters(), lr=self.critic_lr)

        # Initialize learning rate decay schedulers.
        self.actor_scheduler = StepLR(self.actor_optimizer, step_size=2000, gamma=0.5)
        self.critic_scheduler = StepLR(self.critic_optimizer,step_size=2000, gamma=0.5)

        # For storing trajectories
        self.log_probs = []
        self.values = []
        self.rewards = []

        self.drafted_players = []  # List to store drafted players for this agent
        self.total_reward = 0  # Store the total accumulated reward for this agent
        self.total_points = 0  # Store the total accumulated fantasy points for this agent.
        self.position_counts = {"QB": 0, "RB": 0, "WR": 0, "TE": 0}  # Track drafted position counts

    def reset_agent(self):
        """Reset the agent's initial state for a new episode."""
        self.drafted_players = []
        self.total_reward = 0
        self.total_points = 0
        self.position_counts = {"QB": 0, "RB": 0, "WR": 0, "TE": 0}

        # Reset trajectory
        self.log_probs = []
        self.values = []
        self.rewards = []

    def get_state(self, all_agents):
        """Get the current state for the agent. We keep track of the position counts for all teams in the draft."""
        position_counts_tensor = torch.tensor(list(self.position_counts.values()), dtype=torch.float32) # Current agent's state

        # Other teams' position counts
        other_teams_counts = []
        for agent in all_agents:
            if agent.team_id != self.team_id:
                other_teams_counts.extend(agent.position_counts.values())
        other_teams_tensor = torch.tensor(other_teams_counts, dtype=torch.float32)

        # Combine into a single state tensor
        return torch.cat((position_counts_tensor, other_teams_tensor))

    def choose_action(self, state):
        """Choose an action using the actor network."""
        probs = self.actor_network(state)
        action_dist = torch.distributions.Categorical(probs)
        action = action_dist.sample()
        self.log_probs.append(action_dist.log_prob(action))  # Update trajectory
        return action.item()

    def update_networks(self, debug=False):
        """Compute advantages and train actor-critic networks."""
        # Compute discounted return.
        target = 0
        returns = []
        for reward in reversed(self.rewards):
            target = reward + self.discount_factor * target
            returns.insert(0, target)
        returns = torch.tensor(returns)

        # Use returns to compute advantage.
        values = torch.stack(self.values)
        advantages = returns - values.squeeze()

        # Train Actor
        actor_loss = -(torch.stack(self.log_probs) * advantages.detach()).mean()
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor_network.parameters(), max_norm=self.actor_max_norm)
        self.actor_optimizer.step()
        self.actor_scheduler.step()

        # Train Critic
        critic_loss = advantages.pow(2).mean()
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic_network.parameters(), max_norm=self.critic_max_norm)
        self.critic_optimizer.step()
        self.critic_scheduler.step()

        # Debug log to track learning progress.
        if debug:
            print(f"  Agent {self.team_id + 1} updating networks...")
            print(f"  Agent {self.team_id + 1}: Actor Loss = {actor_loss.item():.6f}")
            print(f"  Agent {self.team_id + 1}: Critic Loss = {critic_loss.item():.6f}")
            actor_grad_norm = sum(p.grad.norm().item() for p in self.actor_network.parameters() if p.grad is not None)
            print(f"  Agent {self.team_id + 1}: Actor Gradient Norm = {actor_grad_norm:.6f}")
            critic_grad_norm = sum(p.grad.norm().item() for p in self.critic_network.parameters() if p.grad is not None)
            print(f"  Agent {self.team_id + 1}: Critic Gradient Norm = {critic_grad_norm:.6f}\n")


class FantasyDraft:

    def __init__(self, player_data, num_teams, num_rounds, state_size, action_size, hidden_layers, position_limits):
        """ Initialize the multi-agent draft simulation. """

        self.player_data = player_data.sort_values(by="projected_points", ascending=False)  # Expects a pandas DataFrame.
        self.num_teams = num_teams
        self.num_rounds = num_rounds
        self.draft_order = list(range(num_teams))
        self.position_limits = position_limits
        self.agents = [A2CAgent(team_id=i, state_size=state_size, action_size=action_size, hidden_layers=hidden_layers,
                                  position_limits=position_limits) for i in range(num_teams)]

        # Track rewards.
        self.reward_history = {i: [] for i in range(num_teams)}

        # Cache max possible points by position for reward normalization.
        self.max_points_by_position = player_data.groupby("position")["projected_points"].max()

    def reset_draft(self):
        """Reset the draft for a new episode."""
        self.available_players = self.player_data.copy()
        self.current_round = 0
        self.current_team = 0
        self.draft_order = list(range(self.num_teams))  # Reset draft order
        for agent in self.agents:
            agent.reset_agent()

    def run_episode(self, verbose=False):
        """Run a single episode of the draft."""
        self.reset_draft()
        while self.current_round < self.num_rounds:
            for team in self.draft_order:
                agent = self.agents[team]
                state = agent.get_state(self.agents)

                # Use the actor network to choose an action and the critic network to compute the current value of the state.
                action = agent.choose_action(state)
                value = agent.critic_network(state)

                # Get a list of players who fit the action chosen.
                position = list(self.position_limits.keys())[action]
                available_players = self.available_players[self.available_players["position"] == position]

                # If the action was invalid, punish and lose turn.
                if available_players.empty:
                    reward = -1
                    agent.total_reward += reward
                    agent.values.append(value)
                    agent.rewards.append(reward)
                    continue

                # Draft the best player for the action.
                drafted_player = available_players.iloc[0]
                drafted_player_index = drafted_player.name

                # Add this player to the team.
                agent.total_points += drafted_player["projected_points"]
                agent.drafted_players.append(drafted_player["player_name"] + " " + drafted_player["position"])
                agent.position_counts[drafted_player["position"]] += 1

                # Compute reward and add reward/value to the trajectory.
                reward = self.get_reward(drafted_player, agent)
                agent.total_reward += reward
                agent.values.append(value)
                agent.rewards.append(reward)

                # Remove drafted player from the draft board.
                self.available_players = self.available_players.drop(drafted_player_index)

            self.current_round += 1
            self.draft_order.reverse()

        # Print episode summary
        if verbose:
            sum_rewards, sum_points = 0, 0
            for agent in self.agents:
                sum_rewards += agent.total_reward
                sum_points += agent.total_points
                print(
                    f"  Team {agent.team_id}: Total Reward = {round(agent.total_reward, 2)}, Position Counts = {agent.position_counts}, Drafted Players = {agent.drafted_players} ({round(agent.total_points, 2)} pts)")
            avg_reward = sum_rewards / self.num_teams
            avg_points = sum_points / self.num_teams
            print(f"Average total reward = {avg_reward}, Average total fantasy points = {avg_points}")

    def get_reward(self, drafted_player, agent):
        """Calculate the reward attained for drafting a given player by normalizing it with respect to the maximum
        possible points for that position. If we are exceeding position limits, give negative reward."""
        reward = drafted_player["projected_points"] / self.max_points_by_position[drafted_player["position"]]

        if agent.position_counts[drafted_player["position"]] > self.position_limits[drafted_player["position"]]:
            over_draft_penalty = agent.position_counts[drafted_player["position"]] - self.position_limits[
                drafted_player["position"]]

            if 0 in agent.position_counts.values():
                over_draft_penalty += 1

            reward = -(over_draft_penalty * reward)
            if reward < -1:
                reward = -1

        return reward

    def train(self, num_episodes, verbose=False):
        """Train the agents over multiple episodes."""
        for episode in range(num_episodes):
            self.run_episode()
            for agent in self.agents:
                self.reward_history[agent.team_id].append(agent.total_reward)  # Log rewards for debug purposes.
                agent.update_networks(debug=True if episode == num_episodes - 1 else False)  # Update the agents networks.

            if verbose:
                print(f"Episode {episode + 1}/{num_episodes} completed.")

    def plot_results(self):
        """Plot the learning progress for debug purposes."""
        plt.figure(figsize=(12, 6))
        for team_id, rewards in self.reward_history.items():
            # Compute and plot a moving average for total rewards for each team.
            smoothed_rewards = pd.Series(rewards).rolling(window=50).mean()
            plt.plot(smoothed_rewards, label=f"Team {team_id + 1} Total Rewards")

        plt.title("Total Rewards Over Episodes")
        plt.xlabel("Episode")
        plt.ylabel("Total Reward (Moving Average)")
        plt.legend()
        plt.show()

# Function to run a full training routine.
def run_training_routine():
    # Pandas database of 400 player draft board from FantasyPros.com
    player_data = pd.read_csv("../Best_Ball/Best_Ball_Draft_Board.csv").drop('Unnamed: 0', axis=1).rename(columns={
        "Player": "player_name", "POS": "position", "Fantasy Points": "projected_points"})

    # Setup draft parameters.
    num_teams = 12
    num_rounds = 20
    position_limits = {"QB": 3, "RB": 7, "WR": 8, "TE": 3}

    # Setup neural network structure parameters.
    state_size = len(position_limits) * num_teams   # position_counts + round_number + other_teams_position_counts
    action_size = len(position_limits)
    num_layers = 3  # Number of hidden layers.
    hidden_size = int(np.ceil(
        state_size * (2 / 3))) + action_size  # Dynamically increase hidden neuron number with both action and state sizes.
    hidden_layers = [hidden_size] * num_layers

    # Construct the draft environment.
    draft_simulator = FantasyDraft(player_data, num_teams, num_rounds, state_size, action_size, hidden_layers,
                                   position_limits)

    # Setup training routine.
    actor_max_norms = [1.0, 0.75, 0.5]
    critic_max_norms = [1.5, 1.25, 1.0]
    num_episodes = [2000, 2000, 1000]

    # Run agents through the training routine.
    for phase in range(len(num_episodes)):
        for agent in draft_simulator.agents:
            agent.actor_max_norm = actor_max_norms[phase]
            agent.critic_max_norm = critic_max_norms[phase]

        print(f"\nBeginning training phase {phase + 1}. Number of episodes in this phase is {num_episodes[phase]}.")
        draft_simulator.train(num_episodes=num_episodes[phase], verbose=False)
        print(f"Phase {phase + 1} complete. Running a test draft with no exploitation.")
        draft_simulator.run_episode(verbose=True)

    # Plot rewards and temperatures for debug purposes.
    draft_simulator.plot_results()

    # Save Actor-Critic networks for competitive evaluation.
    for agent in draft_simulator.agents:
        torch.save(agent.actor_network.state_dict(),
                   f"Trained_Agents/A2C_Agents/A2CAgent_{agent.team_id}_Actor_Network.pt")


# run_training_routine()